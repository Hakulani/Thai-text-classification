{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "6420422017 Witsarut Wongsim"
      ],
      "metadata": {
        "id": "0fI8PlyzyuGb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYVar07CqNOH"
      },
      "source": [
        "https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/ <br>\n",
        "https://pytorch.org/hub/huggingface_pytorch-transformers/<br>\n",
        "https://huggingface.co/transformers/quicktour.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary fine tune model\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Here is a summary of the fine-tuning process for the final model:\n",
        "\n",
        "1.Balanced the dataset using RandomOverSampler:\n",
        "Class distribution before RandomOverSampler:\n",
        "\n",
        "0    0.653226\n",
        "1    0.346774\n",
        "Name: label, dtype: float64\n",
        "\n",
        "Class distribution after RandomOverSampler:\n",
        "0    0.5\n",
        "1    0.5\n",
        "\n",
        "Split train dataset into train, validation and test sets + \n",
        "\n",
        "Train 70% (Class 0:50% , Class 1:50%) \n",
        " Val 15%   Test 15%\n",
        "\n",
        "\n",
        "2.max_seq_len = 25\n",
        "\n",
        "3. Chose the pre-trained WangchanBERTa model and froze its weights\n",
        "bert = AutoModel.from_pretrained('poom-sci/WangchanBERTa-finetuned-sentiment')\n",
        "\n",
        "4.batch_size = 32\n",
        "\n",
        "5. Add more layer\n",
        "# dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)    \n",
        "      # dense layer 2\n",
        "\n",
        "      self.fc2 = nn.Linear(512,256)\n",
        "      # dense layer 3 (Output layer)\n",
        "      self.fc3 = nn.Linear(256,2)\n",
        " \n",
        "6. define the optimizer and tuning learing rate\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "\n",
        "7.epochs = 100\n",
        "\n",
        "8. Saved the best model with the lowest validation loss\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "Please find the following attachments for your review:\n",
        "\n",
        "Google Colab notebook file with the final implemented Transformer model, training, and evaluation code\n",
        "Screenshots of the captured results for both training and testing, including the F1 score\n",
        "I would like to thank you for your guidance and support throughout the course of this assignment. Kindly let me know if you need any further information or clarification regarding my submission.\n",
        "\n",
        "Looking forward to your feedback.\n",
        "\n",
        "Best regards\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "7ba7zszpZC_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload csv file spam data v2"
      ],
      "metadata": {
        "id": "ewPSIkp1EMiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GbJTMZ1RY_TC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFOTiqrtNvyy"
      },
      "source": [
        "# Install Transformers Library"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pythainlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWUHFQ4DHHat",
        "outputId": "76eef83a-3578-438f-fd8c-075360a4963f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pythainlp in /usr/local/lib/python3.9/dist-packages (3.1.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.9/dist-packages (from pythainlp) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->pythainlp) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->pythainlp) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->pythainlp) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->pythainlp) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformers in /usr/local/lib/python3.9/dist-packages (4.27.3)"
      ],
      "metadata": {
        "id": "ygvrFLyXJlil"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoOot4MN4q5Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e55a2c8f-e349-43ec-ef9c-3b07fe2983b3"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9lh-o__dZxz"
      },
      "source": [
        "#!pip install transformers==3"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4giRzM7NtHJ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKd-Tj3hOMsZ"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwJrQFQgN_BE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "9364f182-311e-498b-c11a-85e4530093d5"
      },
      "source": [
        "df = pd.read_csv(\"tcas61-2.csv\")#(\"spamdata_v2.csv\")\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label                                               text\n",
              "0      0  มึงกล้าพูดรึป่าวว่าระบบการศึกษามันดีอ่ะ ถุ้ยเฟ...\n",
              "1      0  เบื่อเวลามาโพสตไรแบบนี้ชอบเป็นพวกที่ใช่โปรไฟล์...\n",
              "2      0  พ่อมึงเป็นติ่งรัฐบาลหรอสัส ที่เรียกเก็บตังแพงม...\n",
              "3      0  ใครก็ช่วยลบไอ้นี้ออกจากกลุ่มหน่อยครับ มันมาโพส...\n",
              "4      0                                  เครียดมากอะตอนนี้"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b9e6802d-6509-460c-88ff-8d9d30261fc1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>มึงกล้าพูดรึป่าวว่าระบบการศึกษามันดีอ่ะ ถุ้ยเฟ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>เบื่อเวลามาโพสตไรแบบนี้ชอบเป็นพวกที่ใช่โปรไฟล์...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>พ่อมึงเป็นติ่งรัฐบาลหรอสัส ที่เรียกเก็บตังแพงม...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>ใครก็ช่วยลบไอ้นี้ออกจากกลุ่มหน่อยครับ มันมาโพส...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>เครียดมากอะตอนนี้</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b9e6802d-6509-460c-88ff-8d9d30261fc1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b9e6802d-6509-460c-88ff-8d9d30261fc1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b9e6802d-6509-460c-88ff-8d9d30261fc1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzPPOrVQWiW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8728ceda-dd5f-4ff3-f3ba-feaff02113f7"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(124, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "676DPU1BOPdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2338bff0-a881-45b5-dd0b-b684f227593f"
      },
      "source": [
        "# check class distribution\n",
        "df['label'].value_counts(normalize = True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.653226\n",
              "1    0.346774\n",
              "Name: label, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imbalanced-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PbIk6iIa-GC",
        "outputId": "a8aaa63a-9954-497b-f8c8-f5146144163e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.9/dist-packages (0.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (1.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKfWnApvOoE7"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Split train dataset into train, validation and test sets +RandomOverSampler Train 70  Val 15 Test 15 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['text']\n",
        "y = df['label']\n",
        "print(y.value_counts(normalize=True))\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(X, y, random_state=2018, test_size=0.3, stratify=y)\n",
        "\n",
        "# Apply RandomOverSampler to balance the training dataset\n",
        "ros = RandomOverSampler(random_state=2018)\n",
        "train_text_resampled, train_labels_resampled = ros.fit_resample(pd.DataFrame(train_text), train_labels)\n",
        "\n",
        "# Assign the resampled data back to the original variable names\n",
        "train_text = train_text_resampled.squeeze()\n",
        "train_labels = train_labels_resampled\n",
        "\n",
        "\n",
        "# Display the class distribution after applying RandomOverSampler\n",
        "print(\"\\nClass distribution after RandomOverSampler:\")\n",
        "print(train_labels.value_counts(normalize=True))\n",
        "\n",
        "\n",
        "print('train_text shape =', train_text.shape)\n",
        "print('temp_text shape =', temp_text.shape)\n",
        "\n",
        "# We will use temp_text and temp_labels to create validation and test set\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)\n",
        "\n",
        "print('val_text shape =', val_text.shape)\n",
        "print('test_text shape =', test_text.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0aUQItubOXE",
        "outputId": "1416f094-2c92-4c34-ee82-09e5dc434c05"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    0.653226\n",
            "1    0.346774\n",
            "Name: label, dtype: float64\n",
            "\n",
            "Class distribution after RandomOverSampler:\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: label, dtype: float64\n",
            "train_text shape = (112,)\n",
            "temp_text shape = (38,)\n",
            "val_text shape = (19,)\n",
            "test_text shape = (19,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Train 70  Val 15 Test 15 "
      ],
      "metadata": {
        "id": "wXvCfaa-GXRR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfhSPF5jOWb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "e5b76d61-e228-409c-b7e2-acc3c1a030d0"
      },
      "source": [
        "\"\"\"\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "print('train_text shape =',train_text.shape)\n",
        "print('temp_text shape =',temp_text.shape)\n",
        "\n",
        "# we will use temp_text and temp_labels to create validation and test set\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)\n",
        "\n",
        "print('val_text shape =',val_text.shape)\n",
        "print('test_text shape =',test_text.shape)\n",
        "\"\"\""
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntrain_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \\n                                                                    random_state=2018, \\n                                                                    test_size=0.3, \\n                                                                    stratify=df['label'])\\n\\nprint('train_text shape =',train_text.shape)\\nprint('temp_text shape =',temp_text.shape)\\n\\n# we will use temp_text and temp_labels to create validation and test set\\nval_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \\n                                                                random_state=2018, \\n                                                                test_size=0.5, \\n                                                                stratify=temp_labels)\\n\\nprint('val_text shape =',val_text.shape)\\nprint('test_text shape =',test_text.shape)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7hsdLoCO7uB"
      },
      "source": [
        "# Import BERT Model and BERT Tokenizer\n",
        "https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1kY3gZjO2RE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9520286a-865a-4035-f18c-cf2b753de055"
      },
      "source": [
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('poom-sci/WangchanBERTa-finetuned-sentiment') #bert-base-uncased ,bert-base-multilingual-uncased,Geotrend/bert-base-th-cased'\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "#-Tokenizing (splitting strings in sub-word token strings), converting tokens strings to ids and back, and encoding/decoding (i.e., tokenizing and converting to integers).\n",
        "#-Adding new tokens to the vocabulary in a way that is independent of the underlying structure (BPE, SentencePiece…).\n",
        "#-Managing special tokens \n",
        "tokenizer = BertTokenizerFast.from_pretrained('poom-sci/WangchanBERTa-finetuned-sentiment')#bert-base-uncased , bert-base-multilingual-uncased"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at poom-sci/WangchanBERTa-finetuned-sentiment were not used when initializing CamembertModel: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of CamembertModel were not initialized from the model checkpoint at poom-sci/WangchanBERTa-finetuned-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizerFast'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zOKeOMeO-DT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d5dd9c7-def8-48cc-f7fd-e17f76fd3c88"
      },
      "source": [
        "# sample data\n",
        "text =[\"เบื่อเวลามาโพสตไรแบบนี้ชอบเป็นพวกที่ใช่โปรไฟล์\",\"ครูก็ช่วยลบไอ้นี้ออกจากกลุ่มหน่อยครับมันมาโพส\"] #[\"I see a dog\", \"fine tune\"]\n",
        "\n",
        "# encode text\n",
        "#BatchEncoding holds the output of the tokenizer’s encoding methods and is derived from a Python dictionary. \n",
        "#BatchEncodings behaves just like a standard python dictionary and holds the various model inputs computed by these methods (input_ids, attention_mask)\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)\n",
        "\n",
        "# output\n",
        "print(sent_id)\n",
        "print(sent_id[0])\n",
        "print(sent_id[1])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[5, 16060, 156, 26, 3841, 46, 820, 227, 192, 5679, 12, 762, 18116, 6], [5, 11480, 13932, 988, 829, 44, 630, 171, 4631, 8213, 3841, 6, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]}\n",
            "Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wIYaWI_Prg8"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKwbpeN_PMiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "7a15455a-ef75-42ad-eccd-86764e5e214b"
      },
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 100)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXcswEIRPvGe"
      },
      "source": [
        "max_seq_len = 25"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk5S7DWaP2t6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa6f72d3-7614-4577-e2d3-efee10266de6"
      },
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "# Parameters max_length = The maximum length (in number of tokens) for the inputs to the transformer model\n",
        "\n",
        "# tokenize and encode sequences in the train set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsm8bkRZQTw9"
      },
      "source": [
        "# Convert Integer Sequences to Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR-lXwmzQPd6"
      },
      "source": [
        "#3 input of multihead attention including \n",
        "#1.key = ids\n",
        "#2.query = attention_mask\n",
        "#3.value = label (0,1)\n",
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov1cOBlcRLuk"
      },
      "source": [
        "# Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUy9JKFYQYLp"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2HZc5ZYRV28"
      },
      "source": [
        "# UnFreeze BERT Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ถ้าใช้ Weight จาก Dataseet เราใช้อันนี้ แต่ถ้าใช้ขงเขาไม่ต้องรัน"
      ],
      "metadata": {
        "id": "j1kpi2zhIj_P"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHZ0MC00RQA_"
      },
      "source": [
        "# unfreeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7ahGBUWRi3X"
      },
      "source": [
        "# Define Model Architecture\n",
        "# Solved error: https://stackoverflow.com/questions/66846030/typeerror-linear-argument-input-position-1-must-be-tensor-not-str"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3iEtGyYRd0A"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "# Applies a linear transformation to the incoming data:\n",
        "# Parameters in_features – size of each input sample and out_features – size of each output sample\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "        # layer normalization\n",
        "      self.layer_norm1 = nn.LayerNorm(768)\n",
        "      self.layer_norm2 = nn.LayerNorm(512)\n",
        "\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)    \n",
        "      # dense layer 2\n",
        "\n",
        "      self.fc2 = nn.Linear(512,256)\n",
        "      # dense layer 3 (Output layer)\n",
        "      self.fc3 = nn.Linear(256,2)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask,return_dict=False) #In case use TF 3 version delete  return_dict=False\n",
        "      x = self.layer_norm1(cls_hs)\n",
        "      x = self.fc1(cls_hs)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      #Add layer\n",
        "      x = self.layer_norm2(x)\n",
        "      x = self.fc2(x)  # Added new dense layer\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc3(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aRaDhlCqysz6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBAJJVuJRliv"
      },
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taXS0IilRn9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d72408d-136e-4aaa-e0cc-ef10a9f8a7d7"
      },
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01) # Learning rate  1e-3  = 1x10^-3"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9CDpoMQR_rK"
      },
      "source": [
        "# Find Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izY5xH5eR7Ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ac1d3b-3fbf-4b34-ffe7-72d2d3380c00"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights for unbalanced datasets\n",
        "class_wts = compute_class_weight('balanced', classes= np.unique(train_labels), y= train_labels)\n",
        "\n",
        "print(class_wts)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epochs"
      ],
      "metadata": {
        "id": "WOzh2f2rLQIR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1WvfY2vSGKi"
      },
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 100"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My4CA0qaShLq"
      },
      "source": [
        "# Fine-Tune BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rskLk8R_SahS"
      },
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGXovFDlSxB5"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KZEgxRRTLXG"
      },
      "source": [
        "# Start Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1USGTntS3TS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b1be19-123b-40ea-e5f0-2d046b9b322c"
      },
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.610\n",
            "Validation Loss: 0.428\n",
            "\n",
            " Epoch 2 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.423\n",
            "Validation Loss: 0.265\n",
            "\n",
            " Epoch 3 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.290\n",
            "Validation Loss: 0.161\n",
            "\n",
            " Epoch 4 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.189\n",
            "Validation Loss: 0.102\n",
            "\n",
            " Epoch 5 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.140\n",
            "Validation Loss: 0.069\n",
            "\n",
            " Epoch 6 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.080\n",
            "Validation Loss: 0.057\n",
            "\n",
            " Epoch 7 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.050\n",
            "Validation Loss: 0.059\n",
            "\n",
            " Epoch 8 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.044\n",
            "Validation Loss: 0.058\n",
            "\n",
            " Epoch 9 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.041\n",
            "Validation Loss: 0.056\n",
            "\n",
            " Epoch 10 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.026\n",
            "Validation Loss: 0.050\n",
            "\n",
            " Epoch 11 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.020\n",
            "Validation Loss: 0.044\n",
            "\n",
            " Epoch 12 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.014\n",
            "Validation Loss: 0.041\n",
            "\n",
            " Epoch 13 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.015\n",
            "Validation Loss: 0.035\n",
            "\n",
            " Epoch 14 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.010\n",
            "Validation Loss: 0.026\n",
            "\n",
            " Epoch 15 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.007\n",
            "Validation Loss: 0.022\n",
            "\n",
            " Epoch 16 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.026\n",
            "Validation Loss: 0.019\n",
            "\n",
            " Epoch 17 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.012\n",
            "Validation Loss: 0.015\n",
            "\n",
            " Epoch 18 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.018\n",
            "Validation Loss: 0.014\n",
            "\n",
            " Epoch 19 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.034\n",
            "Validation Loss: 0.013\n",
            "\n",
            " Epoch 20 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.009\n",
            "Validation Loss: 0.013\n",
            "\n",
            " Epoch 21 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.011\n",
            "Validation Loss: 0.010\n",
            "\n",
            " Epoch 22 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.056\n",
            "Validation Loss: 0.007\n",
            "\n",
            " Epoch 23 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.006\n",
            "\n",
            " Epoch 24 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.005\n",
            "Validation Loss: 0.006\n",
            "\n",
            " Epoch 25 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.048\n",
            "Validation Loss: 0.006\n",
            "\n",
            " Epoch 26 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.016\n",
            "Validation Loss: 0.007\n",
            "\n",
            " Epoch 27 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.005\n",
            "Validation Loss: 0.009\n",
            "\n",
            " Epoch 28 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.007\n",
            "Validation Loss: 0.010\n",
            "\n",
            " Epoch 29 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.006\n",
            "Validation Loss: 0.012\n",
            "\n",
            " Epoch 30 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.006\n",
            "Validation Loss: 0.013\n",
            "\n",
            " Epoch 31 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.046\n",
            "Validation Loss: 0.014\n",
            "\n",
            " Epoch 32 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.015\n",
            "\n",
            " Epoch 33 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.016\n",
            "\n",
            " Epoch 34 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.032\n",
            "\n",
            " Epoch 35 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.030\n",
            "Validation Loss: 0.017\n",
            "\n",
            " Epoch 36 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.026\n",
            "Validation Loss: 0.017\n",
            "\n",
            " Epoch 37 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.017\n",
            "\n",
            " Epoch 38 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.016\n",
            "Validation Loss: 0.016\n",
            "\n",
            " Epoch 39 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.039\n",
            "Validation Loss: 0.011\n",
            "\n",
            " Epoch 40 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.028\n",
            "Validation Loss: 0.007\n",
            "\n",
            " Epoch 41 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.005\n",
            "\n",
            " Epoch 42 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.007\n",
            "Validation Loss: 0.005\n",
            "\n",
            " Epoch 43 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.005\n",
            "\n",
            " Epoch 44 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.027\n",
            "Validation Loss: 0.003\n",
            "\n",
            " Epoch 45 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.003\n",
            "\n",
            " Epoch 46 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 47 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 48 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 49 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 50 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 51 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.025\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 52 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 53 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.008\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 54 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 55 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 56 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.009\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 57 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.021\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 58 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 59 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.030\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 60 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.001\n",
            "\n",
            " Epoch 61 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.020\n",
            "Validation Loss: 0.001\n",
            "\n",
            " Epoch 62 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.001\n",
            "\n",
            " Epoch 63 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.007\n",
            "Validation Loss: 0.001\n",
            "\n",
            " Epoch 64 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.040\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 65 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 66 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 67 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 68 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 69 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 70 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 71 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 72 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 73 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.059\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 74 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 75 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 76 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 77 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 78 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 79 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 80 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.020\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 81 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.022\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 82 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 83 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 84 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 85 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.005\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 86 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 87 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.002\n",
            "\n",
            " Epoch 88 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.055\n",
            "Validation Loss: 0.019\n",
            "\n",
            " Epoch 89 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.050\n",
            "\n",
            " Epoch 90 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.076\n",
            "\n",
            " Epoch 91 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.056\n",
            "Validation Loss: 0.094\n",
            "\n",
            " Epoch 92 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.103\n",
            "\n",
            " Epoch 93 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.107\n",
            "\n",
            " Epoch 94 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.107\n",
            "\n",
            " Epoch 95 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.105\n",
            "\n",
            " Epoch 96 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.103\n",
            "\n",
            " Epoch 97 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.101\n",
            "\n",
            " Epoch 98 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.099\n",
            "\n",
            " Epoch 99 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.096\n",
            "\n",
            " Epoch 100 / 100\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.001\n",
            "Validation Loss: 0.093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yrhUc9kTI5a"
      },
      "source": [
        "# Load Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OacxUyizS8d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b51251c1-7ca7-4bf4-aa2f-a7b49b26b766"
      },
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4SVftkkTZXA"
      },
      "source": [
        "# Get Predictions for Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZl0SZmFTRQA"
      },
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(train_seq.to(device), train_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms1ObHZxTYSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b99e44e-869e-4f8c-8f8b-d2c524486e16"
      },
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(train_y, preds))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        56\n",
            "           1       1.00      1.00      1.00        56\n",
            "\n",
            "    accuracy                           1.00       112\n",
            "   macro avg       1.00      1.00      1.00       112\n",
            "weighted avg       1.00      1.00      1.00       112\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqzLS7rHTp4T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "4d8a4f6a-b2a2-4833-b930-73b154fabc02"
      },
      "source": [
        "# confusion matrix\n",
        "pd.crosstab(train_y, preds)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "col_0   0   1\n",
              "row_0        \n",
              "0      56   0\n",
              "1       0  56"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-317499ab-ff83-43f4-a0dc-ece35a5ccef8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-317499ab-ff83-43f4-a0dc-ece35a5ccef8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-317499ab-ff83-43f4-a0dc-ece35a5ccef8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-317499ab-ff83-43f4-a0dc-ece35a5ccef8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "BLlpy1B1kELW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "kScKoW_djfuZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBpJhiQDjmV6",
        "outputId": "2756267b-a94a-4f1d-9879-740d5edc4082"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        13\n",
            "           1       1.00      1.00      1.00         6\n",
            "\n",
            "    accuracy                           1.00        19\n",
            "   macro avg       1.00      1.00      1.00        19\n",
            "weighted avg       1.00      1.00      1.00        19\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix\n",
        "pd.crosstab(test_y, preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "7C7BKZBvjpko",
        "outputId": "41d76f97-797a-4525-88f4-d9d12a218b8b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "col_0   0  1\n",
              "row_0       \n",
              "0      13  0\n",
              "1       0  6"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2fd40e82-06e4-4ad3-aafb-a28ff976e01d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2fd40e82-06e4-4ad3-aafb-a28ff976e01d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2fd40e82-06e4-4ad3-aafb-a28ff976e01d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2fd40e82-06e4-4ad3-aafb-a28ff976e01d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qMOg7XDVj3wL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}